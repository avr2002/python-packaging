## Testing

- **Tests should be black box as much as possible:**
  - When optimizing/changing the code implementation that produces the same
    output as before, the test cases for that function should not get affected by
    this change. This what we mean by **balck box**.

  - **Tests should always test the behaviour of code, instead of implementation of
    behaviours.**

  - Tests that are not inline with above, are called **brittle tests**.


- **Test Case Universe**
  - The universe is the set of all possible test cases or all possible inputs
    that we could run through a function in testing.

  - But there are many tests for which it is infeasible to think/write all the
    possible test cases.

- **Unit Test**
  - A unit test is a type of test that tests one very specific thing or set differently.
    It tests a single unit of your code base that could be a single unit of your app's
    functionality, or it could be a single unit of code, like literally a single
    function or a single class.

  - **Why unit test?**
    - The behind idea that is if your test is only testing one thing and then that test
      suddenly fails to run, you have a pretty good idea of what caused that test to fail.

    - Whereas if you have a complex test that's running 20 different functions that you wrote,
      and then the test fails, you don't know which of those 20 functions failed or if some
      interaction between those functions caused them to fail.

  - The convention is that for each piece/unit of code we write a test specific to that unit.

  - So, a good way to organize our tests would be to have a parallel folder structure that
    mirrors what our packages folder file structure, in our test folder too.


- **Test Coverage**
  - In blak box testing, when we write our tests and our tests do try to test for behaviors 
    instead of implementations of behaviors.
  
  - So we probably want enough tests at least to check in an automated way that 
    all of the core behaviors of our code base are still intact.

  - Tests that tests the core functionality of our package/app, etc. are much more important
    to implement than the more granular unit tests for private modules in the codebase that
    aren't publicly exposed.

  - However, tests that do check the behaviors on invisible or non exposed parts of your codebase still 
    do have some value, and that is if you have tests, then people who go to modify your code will be 
    less afraid to try and modify your code because they'll be able to check to see if their modifications 
    actually broke the particular unit of code that they're working on.

  - One check or test that is nice to implement is to check that every single line of our source 
    code gets executed.
  
  - **Test Coverage reports does exactly the above, it tells the percentage of lines of your program 
    that actually got executed.**

  - Tools to generate coverage report:
    - [Coverage.py](https://coverage.readthedocs.io/en/7.5.1/)
    - [pytest-cov](https://pytest-cov.readthedocs.io/en/latest/): Wrapper around coverage.py for pytest
      - `pip install pytest-cov`

  - Generating Coverage Report using `pytest-cov`:
    ```bash
    # get help: pytest --help

    python -m pytest "${THIS_DIR}/tests/" \
        --cov "${THIS_DIR}/packaging_demo" \
        --cov-report html

    # python -m pytest <tests_dir> --cov <source_code_dir> --cov-report <report_type>

    # To see the html report, run a local python server
    python -m http.server -d "<path to htmlcov dir generated by pytest-cov>"
    ```

    ![test-coverage](../packaging_demo/assets/test-coverage.png)


  - Optimizing for 100% Test Coverage is not always helpful
  
    - It is possible that you could have 100% test coverage and that your tests not 
      actually be testing the functionality of the code.

    - So you could have 100% test coverage and your program could be completely broken, 
      like, yes, we executed it, but at the minimum, all executing does is verify that 
      we don't have syntax errors or something, or that we don't run into a case where 
      we raise an exception.

    - There is a marginal increase in number of tests that we may have to write to attain
      100% test coverage of our codebase. And it's not worth developer's time because along
      with our source code we would have to maintain our test suite too.

    - We can enforce a minimum threshold for our test coverage, say 75% or 80% for our codebase.
      The threshold is subjective to each codebase and decided by the developer.

      ```bash
      pytest --help
      >>> --cov-fail-under=MIN  Fail if the total coverage is less than MIN.
      ```